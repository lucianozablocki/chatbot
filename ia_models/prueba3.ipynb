{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prueba3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OMVzqXWlXlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch as tr\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import pandas\n",
        "class BOW():\n",
        "    def __init__(self,dataset,strip_accents,stoplist,weighting,ngram = None):\n",
        "        #autocorregir -> lematizar -> borrar signos, carac especiales, stopwords, pasar a minuscula\n",
        "        #x_text_auto = preprocesamiento.Autocorrector(dataset.values)\n",
        "        #x_text_lem = preprocesamiento.Lematizar(dataset)\n",
        "        #x_text_lem = x_text_lem[:,1]\n",
        "        #x_text_lem = dataset[:,1] esto anda\n",
        "        if weighting:\n",
        "            if ngram: \n",
        "                vectorizer = TfidfVectorizer(strip_accents=strip_accents,stop_words=stoplist,ngram_range=ngram) \n",
        "            else:\n",
        "                vectorizer = TfidfVectorizer(strip_accents=strip_accents,stop_words=stoplist)   \n",
        "        else:\n",
        "            if ngram:\n",
        "                vectorizer = CountVectorizer(strip_accents=strip_accents,stop_words=stoplist,ngram_range=ngram)     \n",
        "            else: \n",
        "                vectorizer = CountVectorizer(strip_accents=strip_accents,stop_words=stoplist)\n",
        "        #X = vectorizer.fit_transform(x_text_lem)  \n",
        "        X = vectorizer.fit_transform(dataset)\n",
        "        self.vectorizer = vectorizer\n",
        "        self.X = X\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return(self.vectorizer.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmsOCxPplIH-",
        "colab_type": "code",
        "outputId": "cae5f85c-a6d4-41f5-9d83-891782232833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "#from torchvision import datasets\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "#from prueba2 import MLP\n",
        "!pip install skorch\n",
        "import skorch\n",
        "from skorch import NeuralNetClassifier,NeuralNet\n",
        "from sklearn.model_selection import RandomizedSearchCV,train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from mpl_toolkits import mplot3d\n",
        "import scipy.interpolate as interp\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting skorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/1e/cc4e1f23cd1faab06672f309e0857294aaa80c5f84670f4d3d19b08ab10b/skorch-0.7.0-py3-none-any.whl (105kB)\n",
            "\r\u001b[K     |███                             | 10kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 20kB 27.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 30kB 32.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 40kB 36.7MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 51kB 38.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 61kB 41.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 71kB 43.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 81kB 44.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 92kB 46.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 102kB 48.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 48.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.3.3)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.21.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.17.4)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.8.6)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (4.28.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->skorch) (0.14.1)\n",
            "Installing collected packages: skorch\n",
            "Successfully installed skorch-0.7.0\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8HbLhFZmVoG",
        "colab_type": "code",
        "outputId": "d772fdac-44af-439b-c824-d2bae253dece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#cargar csv con preguntas preprocesadas\n",
        "print(\"-----------CARGANDO/CREANDO PREGUNTAS PREPROCESADAS--------------\")\n",
        "#correctedData = preprocesamiento.preprocesar(dataset.values,1) #Dataset lematizado, descomentar en caso de crear otro dataset \n",
        "correctedData = pd.read_csv(\"preprocessedQuestions_stem_completadas.csv\",delimiter=',',header=None) #comentar esta linea en caso de descomentar la anterior\n",
        "print(\"----------FINISHED CARGANDO/CREANDO PREGUNTAS PREPROCESADAS-------------\\n\")\n",
        "\n",
        "#obtener datos utiles sobre el dataset\n",
        "labels = correctedData.values[:,0]\n",
        "print(\"Shape de corrected data: \",correctedData.shape)\n",
        "cantidad_labels = correctedData.values[len(correctedData.values)-1,0] + 1\n",
        "cantidad_preg = correctedData.shape[0]\n",
        "print(\"Cantidad de clases: \",cantidad_labels)\n",
        "print(\"Cantidad de patrones: \",cantidad_preg)\n",
        "print(\"Lista de clases para cada pregunta :\",labels)\n",
        "correctedData = correctedData.values\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------CARGANDO/CREANDO PREGUNTAS PREPROCESADAS--------------\n",
            "----------FINISHED CARGANDO/CREANDO PREGUNTAS PREPROCESADAS-------------\n",
            "\n",
            "Shape de corrected data:  (1527, 2)\n",
            "Cantidad de clases:  106\n",
            "Cantidad de patrones:  1527\n",
            "Lista de clases para cada pregunta : [0 0 0 ... 105 105 105]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU4ALjUanLmF",
        "colab_type": "code",
        "outputId": "dd7abe47-128f-4654-b7fc-98f1aa9b5f6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stoplist = stopwords.words('spanish')\n",
        "text = correctedData[:,1]\n",
        "bow_unigram = BOW(text,'ascii',stoplist,weighting=False, ngram=(1,3))\n",
        "\n",
        "print(\"-------------CREANDO Ytest e Ytrain (groundtruth de cada subconjunto), Xtext_test y Xtext_train-------------------\")\n",
        "Y = np.zeros((cantidad_preg),dtype=np.int64)\n",
        "for i in range(cantidad_preg):\n",
        "    Y[i] = correctedData[i,0]\n",
        "\n",
        "Y = torch.from_numpy(Y)\n",
        "print(Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "-------------CREANDO Ytest e Ytrain (groundtruth de cada subconjunto), Xtext_test y Xtext_train-------------------\n",
            "tensor([  0,   0,   0,  ..., 105, 105, 105])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFyJ-wfewIw-",
        "colab_type": "code",
        "outputId": "df8f11bb-987f-424f-efa8-45c7634bc13f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "D_in = bow_unigram.X.shape[1]\n",
        "D_out = cantidad_labels\n",
        "print(D_in)\n",
        "print(D_out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5254\n",
            "106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgU2DqQinVCQ",
        "colab_type": "code",
        "outputId": "bc181ab9-1205-4c40-e9b3-8569ae362012",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "print(\"-----------------INIT TRAINING PROCESS---------------------\")\n",
        "parameters = {    \n",
        "'hidden_layer_sizes' : [(4000,1000,500),(4000),(4000,1500,300),(5000,1000)],\n",
        "#'n_layers' : [1,2,3,4],\n",
        "#'dropout' : [0.2,0.3,0.4,0.5,0.6],\n",
        "#'bidirectional' : [True,False],\n",
        "'max_iter' : [50,100,200,300],\n",
        "'alpha' : [0.00001,0.0001,0.001,0.1,0.2],\n",
        "'batch_size' : [1500,750,500,300]\n",
        "}\n",
        "\n",
        "net = MLPClassifier(hidden_layer_sizes=(100))\n",
        "candidatos = 10\n",
        "gs = RandomizedSearchCV(net,parameters,verbose=2,n_jobs=-1,cv=10,scoring='balanced_accuracy',n_iter=candidatos)\n",
        "print(type(bow_unigram.X))\n",
        "X_numpy = bow_unigram.X.toarray()\n",
        "print(type(X_numpy))\n",
        "print(type(Y))\n",
        "Y_numpy = Y.numpy()\n",
        "X_train,X_test,y_train,y_test = train_test_split(X_numpy,Y_numpy,shuffle=True,stratify=Y,test_size=0.1,random_state=12)\n",
        "\n",
        "Y_numpy = np.asarray(y_train)\n",
        "\n",
        "gs.fit(X_train,Y_numpy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------INIT TRAINING PROCESS---------------------\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'torch.Tensor'>\n",
            "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 9 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
            "  % (min_groups, self.n_splits)), Warning)\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fseYTujqt2D5",
        "colab_type": "code",
        "outputId": "c85d90a4-71aa-480c-e163-7e27f5dc2ef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "source": [
        "hidden_layer_sizes = []\n",
        "max_epochs = []\n",
        "alpha= []\n",
        "batch_size=[]\n",
        "std=[]\n",
        "score=[]\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "def report(results, n_top=3):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
        "                results['mean_test_score'][candidate],\n",
        "                results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")\n",
        "            score.append(results['mean_test_score'][candidate])\n",
        "            std.append(results['std_test_score'][candidate])\n",
        "            hidden_layer_sizes.append(results['params'][candidate]['hidden_layer_sizes'])\n",
        "            batch_size.append(results['params'][candidate]['batch_size'])\n",
        "            alpha.append(results['params'][candidate]['alpha'])\n",
        "            max_epochs.append(results['params'][candidate]['max_iter'])\n",
        "\n",
        "\n",
        "report(gs.cv_results_,candidatos)  \n",
        "\n",
        "print(\"Batch size: \",batch_size)\n",
        "print(\"Hidden Dim: \",hidden_layer_sizes)\n",
        "print(\"Max epochs: \",max_epochs)\n",
        "print(\"Alpha: \",alpha)\n",
        "print(\"Score: \",score)\n",
        "print(\"std: \",std)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model with rank: 1\n",
            "Mean validation score: 0.776 (std: 0.037)\n",
            "Parameters: {'max_iter': 50, 'hidden_layer_sizes': 500, 'batch_size': 300, 'alpha': 0.001}\n",
            "\n",
            "Model with rank: 2\n",
            "Mean validation score: 0.773 (std: 0.037)\n",
            "Parameters: {'max_iter': 100, 'hidden_layer_sizes': 550, 'batch_size': 300, 'alpha': 1e-05}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.773 (std: 0.032)\n",
            "Parameters: {'max_iter': 50, 'hidden_layer_sizes': 600, 'batch_size': 300, 'alpha': 0.001}\n",
            "\n",
            "Model with rank: 4\n",
            "Mean validation score: 0.771 (std: 0.030)\n",
            "Parameters: {'max_iter': 75, 'hidden_layer_sizes': 550, 'batch_size': 300, 'alpha': 0.0001}\n",
            "\n",
            "Model with rank: 5\n",
            "Mean validation score: 0.769 (std: 0.027)\n",
            "Parameters: {'max_iter': 100, 'hidden_layer_sizes': 500, 'batch_size': 300, 'alpha': 1e-05}\n",
            "\n",
            "Model with rank: 6\n",
            "Mean validation score: 0.768 (std: 0.030)\n",
            "Parameters: {'max_iter': 50, 'hidden_layer_sizes': 500, 'batch_size': 300, 'alpha': 1e-05}\n",
            "\n",
            "Batch size:  [300, 300, 300, 300, 300, 300]\n",
            "Hidden Dim:  [500, 550, 600, 550, 500, 500]\n",
            "Max epochs:  [50, 100, 50, 75, 100, 50]\n",
            "Alpha:  [0.001, 1e-05, 0.001, 0.0001, 1e-05, 1e-05]\n",
            "Score:  [0.7756976194119405, 0.7734115493990847, 0.7725699903426807, 0.7708455357644967, 0.7685351075971846, 0.7682826749111662]\n",
            "std:  [0.0365007466112037, 0.03714253350267622, 0.03167687001536018, 0.029800583012826756, 0.02681675849963973, 0.02977667866199823]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnJwm-UbuCls",
        "colab_type": "code",
        "outputId": "d103129b-2f65-4145-c2fb-dcb7c21f0165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hidden_dim = hidden_layer_sizes\n",
        "num_layers = list(map(lambda x: 1 if type(x)==int else len(x), hidden_dim))\n",
        "hidden_dim = list(map(lambda x: x if type(x)==int else np.average(x), hidden_dim))\n",
        "#num_layers = [1,1,1,2,2,2,1,1,2,1,1,3,2,3,1]\n",
        "ejex = hidden_dim #Da error esto, uso promedio de neuronas o qué onda? \n",
        "ejey = num_layers\n",
        "ejez = score\n",
        "\n",
        "plotx,ploty, = np.meshgrid(np.linspace(np.min(ejex),np.max(ejex),10),\\\n",
        "                           np.linspace(np.min(ejey),np.max(ejey),10))\n",
        "plotz = interp.griddata((ejex,ejey),ejez,(plotx,ploty),method='cubic')\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.view_init(35,40)\n",
        "ax.set_xlabel('Neuronas')\n",
        "ax.set_ylabel('Capas')\n",
        "ax.set_zlabel('Precisión')\n",
        "ax.set_title('Precisión en función de la arquitectura')\n",
        "\n",
        "surf = ax.plot_surface(plotx,ploty,plotz,vmin=np.nanmin(ejez),vmax = np.nanmax(ejez),cstride=1,rstride=1,cmap='viridis')\n",
        "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "plt.savefig(\"prueba3_Neuronas_Capas_Precision.svg\")\n",
        "#/////////////////////////////\n",
        "#////////////////////////////\n",
        "\n",
        "ejex = hidden_dim\n",
        "ejey = num_layers\n",
        "ejez = std\n",
        "\n",
        "plotx,ploty, = np.meshgrid(np.linspace(np.min(ejex),np.max(ejex),10),\\\n",
        "                           np.linspace(np.min(ejey),np.max(ejey),10))\n",
        "plotz = interp.griddata((ejex,ejey),ejez,(plotx,ploty),method='cubic')\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.view_init(35,40)\n",
        "ax.set_xlabel('Neuronas')\n",
        "ax.set_ylabel('Capas')\n",
        "ax.set_zlabel('Std')\n",
        "ax.set_title('Std en función de la arquitectura')\n",
        "\n",
        "surf = ax.plot_surface(plotx,ploty,plotz,vmin=np.nanmin(ejez),vmax = np.nanmax(ejez),cstride=1,rstride=1,cmap='viridis')\n",
        "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "plt.savefig(\"prueba3_Neuronas_Capas_STD.svg\")\n",
        "#/////////////////////////////\n",
        "#////////////////////////////\n",
        "\n",
        "ejex = max_epochs\n",
        "ejey = batch_size\n",
        "ejez = score\n",
        "\n",
        "plotx,ploty, = np.meshgrid(np.linspace(np.min(ejex),np.max(ejex),10),\\\n",
        "                           np.linspace(np.min(ejey),np.max(ejey),10))\n",
        "plotz = interp.griddata((ejex,ejey),ejez,(plotx,ploty),method='cubic')\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.view_init(35,120)\n",
        "ax.set_xlabel('Epocas')\n",
        "ax.set_ylabel('Batch')\n",
        "ax.set_zlabel('Precisión')\n",
        "ax.set_title('Precisión en función de tamaño de batch y épocas')\n",
        "\n",
        "surf = ax.plot_surface(plotx,ploty,plotz,vmin=np.nanmin(ejez),vmax = np.nanmax(ejez),cstride=1,rstride=1,cmap='viridis')\n",
        "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "plt.savefig(\"prueba3_Epocas_Batch_Precision.svg\")\n",
        "\n",
        "#/////////////////////////////\n",
        "#////////////////////////////\n",
        "\n",
        "ejex = max_epochs\n",
        "ejey = batch_size\n",
        "ejez = std\n",
        "\n",
        "plotx,ploty, = np.meshgrid(np.linspace(np.min(ejex),np.max(ejex),10),\\\n",
        "                           np.linspace(np.min(ejey),np.max(ejey),10))\n",
        "plotz = interp.griddata((ejex,ejey),ejez,(plotx,ploty),method='cubic')\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.view_init(35,120)\n",
        "ax.set_xlabel('Epocas')\n",
        "ax.set_ylabel('Batch')\n",
        "ax.set_zlabel('Std')\n",
        "ax.set_title('Std en función de tamaño de batch y épocas')\n",
        "\n",
        "surf = ax.plot_surface(plotx,ploty,plotz,vmin=np.nanmin(ejez),vmax = np.nanmax(ejez),cstride=1,rstride=1,cmap='viridis')\n",
        "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "plt.savefig(\"prueba3_Epocas_Batch_STD.svg\")\n",
        "\n",
        "#/////////////////////////////\n",
        "#////////////////////////////\n",
        "\n",
        "ejex = alpha\n",
        "ejey = max_epochs\n",
        "ejez = score\n",
        "\n",
        "plotx,ploty, = np.meshgrid(np.linspace(np.min(ejex),np.max(ejex),10),\\\n",
        "                           np.linspace(np.min(ejey),np.max(ejey),10))\n",
        "plotz = interp.griddata((ejex,ejey),ejez,(plotx,ploty),method='cubic')\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.view_init(35,120)\n",
        "ax.set_xlabel('Alpha')\n",
        "ax.set_ylabel('Epocas')\n",
        "ax.set_zlabel('Precisión')\n",
        "ax.set_title('Precisión en función de la regularización L2')\n",
        "\n",
        "surf = ax.plot_surface(plotx,ploty,plotz,vmin=np.nanmin(ejez),vmax = np.nanmax(ejez),cstride=1,rstride=1,cmap='viridis')\n",
        "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "plt.savefig(\"prueba3_Alpha_Epocas_Precision.svg\")\n",
        "\n",
        "probs = gs.best_estimator_.predict_proba(X_test)\n",
        "#print(probs.shape)\n",
        "\n",
        "# get training and validation loss\n",
        "#epochs = [i for i in range(len(gs.best_estimator_.history))]\n",
        "epochs = [i for i in range(max_epochs[0])]\n",
        "#train_loss = gs.best_estimator_.history[:,'train_loss']\n",
        "train_loss = gs.best_estimator_.loss_curve_\n",
        "\n",
        "#valid_loss = gs.best_estimator_.history[:,'valid_loss']\n",
        "acc = balanced_accuracy_score(y_test_tensor,np.argmax(probs,axis=1))\n",
        "print(\"tasa de acierto obtenida: \",acc)\n",
        "fig1 = plt.figure()\n",
        "plt.plot(epochs,train_loss,'g-')\n",
        "#plt.plot(epochs,valid_loss,'r-')\n",
        "plt.title('Curvas del error en el entrenamiento')\n",
        "plt.xlabel('Epocas')\n",
        "plt.ylabel('Error (Cross Entropy Loss)')\n",
        "plt.legend(['Entrenamiento'])\n",
        "plt.savefig(\"prueba3_TrainVsVal.svg\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "QhullError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mQhullError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-6517298e1fe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplotx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mploty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeshgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mejex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mejex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                           \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mejey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mejey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mplotz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgriddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mejex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mejey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mejez\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplotx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mploty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'3d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/interpolate/ndgriddata.py\u001b[0m in \u001b[0;36mgriddata\u001b[0;34m(points, values, xi, method, fill_value, rescale)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cubic'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         ip = CloughTocher2DInterpolator(points, values, fill_value=fill_value,\n\u001b[0;32m--> 226\u001b[0;31m                                         rescale=rescale)\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32minterpnd.pyx\u001b[0m in \u001b[0;36mscipy.interpolate.interpnd.CloughTocher2DInterpolator.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mqhull.pyx\u001b[0m in \u001b[0;36mscipy.spatial.qhull.Delaunay.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mqhull.pyx\u001b[0m in \u001b[0;36mscipy.spatial.qhull._Qhull.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mQhullError\u001b[0m: QH6154 Qhull precision error: Initial simplex is flat (facet 1 is coplanar with the interior point)\n\nWhile executing:  | qhull d Qc Qt Q12 Qz Qbb\nOptions selected for Qhull 2015.2.r 2016/01/18:\n  run-id 1513130144  delaunay  Qcoplanar-keep  Qtriangulate  Q12-no-wide-dup\n  Qz-infinity-point  Qbbound-last  _pre-merge  _zero-centrum  Qinterior-keep\n  Pgood  _max-width 1e+02  Error-roundoff 6e-13  _one-merge 4.2e-12\n  Visible-distance 1.2e-12  U-coplanar-distance 1.2e-12  Width-outside 2.4e-12\n  _wide-facet 7.3e-12\n\nprecision problems (corrected unless 'Q0' or an error)\n      2 flipped facets\n\nThe input to qhull appears to be less than 3 dimensional, or a\ncomputation has overflowed.\n\nQhull could not construct a clearly convex simplex from points:\n- p1(v3): 5.5e+02     1    36\n- p6(v2): 5.3e+02     1 1e+02\n- p2(v1): 6e+02     1    75\n- p0(v0): 5e+02     1     0\n\nThe center point is coplanar with a facet, or a vertex is coplanar\nwith a neighboring facet.  The maximum round off error for\ncomputing distances is 6e-13.  The center point, facets and distances\nto the center point are as follows:\n\ncenter point    545.8        1    52.83\n\nfacet p6 p2 p0 distance=    0\nfacet p1 p2 p0 distance=    0\nfacet p1 p6 p0 distance=    0\nfacet p1 p6 p2 distance=    0\n\nThese points either have a maximum or minimum x-coordinate, or\nthey maximize the determinant for k coordinates.  Trial points\nare first selected from points that maximize a coordinate.\n\nThe min and max coordinates for each dimension are:\n  0:       500       600  difference=  100\n  1:         1         1  difference=    0\n  2:         0       100  difference=  100\n\nIf the input should be full dimensional, you have several options that\nmay determine an initial simplex:\n  - use 'QJ'  to joggle the input and make it full dimensional\n  - use 'QbB' to scale the points to the unit cube\n  - use 'QR0' to randomly rotate the input for different maximum points\n  - use 'Qs'  to search all points for the initial simplex\n  - use 'En'  to specify a maximum roundoff error less than 6e-13.\n  - trace execution with 'T3' to see the determinant for each point.\n\nIf the input is lower dimensional:\n  - use 'QJ' to joggle the input and make it full dimensional\n  - use 'Qbk:0Bk:0' to delete coordinate k from the input.  You should\n    pick the coordinate with the least range.  The hull will have the\n    correct topology.\n  - determine the flat containing the points, rotate the points\n    into a coordinate plane, and delete the other coordinates.\n  - add one or more points to make the input full dimensional.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYqQknNJQeaS",
        "colab_type": "code",
        "outputId": "daa20aab-eff4-4bbb-d02e-bc109a12822d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "acc = balanced_accuracy_score(y_test_tensor,np.argmax(probs,axis=1))\n",
        "print(\"tasa de acierto obtenida: \",acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tasa de acierto obtenida:  0.7987421383647799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2YcYX3Pl2nU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}